## Text Mining

What is Text Mining?

The Process of distilling actionable insights from text

### Text Mining WorkFlow

1. Problem Definition and Specific goals
2. Identify text to be collected
3. Text Organization
4. Feature Extraction
5. Analysis
6. Reach an Insight, conclusion.

### Semantic Parsing vs Bag of Words

Semantic Parsing: We care about word type and order. This creates features lots of features. Single word can be part of a sentence or a Noun or a proper noun etc. Hence there are 3 features, hence feature rich. Therefore Semantic parsing follows the tree structure.

Bag of words: Does not care about word type or order. Words are considered as tokens.

Sometimes we can find out the author's intent and main ideas just by looking at the most common words. At its heart, bag of words text mining represents a way to count terms, or n-grams, across a collection of documents. 

```{r,warning=FALSE,message=FALSE}
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."

# Load qdap
require(qdap)

# Find the 10 most frequent terms: term_count
term_count <- freq_terms(text,10)

# Plot term_count
plot(term_count)

```

### Building Corpus from vector and dataframe

#### 1. Vector

```{r,warning=FALSE,message=FALSE}
# Import text data
require(RCurl)

URLFile <- getURL("https://raw.githubusercontent.com/diegonogare/DataScience/master/Text%20Mining/coffee.csv")
tweets <- read.csv(text = URLFile,stringsAsFactors = FALSE)

# View the structure of tweets
str(tweets)

# Print out the number of rows in tweets
nrow(tweets)

# Isolate text from tweets: coffee_tweets
coffee_tweets <- tweets$text

head(coffee_tweets)

```

Now you've loaded your text data as a vector called coffee_tweets. Your next step is to convert this vector containing the text data to a corpus.

There are two kinds of the corpus data type, the permanent corpus, PCorpus, and the volatile corpus, VCorpus. In essence, the difference between the two has to do with how the collection of documents is stored in your computer. In this course, we will use the volatile corpus, which is held in your computer's RAM rather than saved to disk, just to be more memory efficient.

To make a volatile corpus, R needs to interpret each element in our vector of text, coffee_tweets, as a document. And the tm package provides what are called Source functions to do just that! In this exercise, we'll use a Source function called VectorSource() because our text data is contained in a vector. The output of this function is called a Source object.

```{r,warning=FALSE,message=FALSE}
# Load tm
require(tm)

# Make a vector source: coffee_source
coffee_source <- VectorSource(coffee_tweets)

```

Now that we've converted our vector to a Source object, we pass it to another tm function, VCorpus(), to create our volatile corpus.

The VCorpus object is a nested list, or list of lists. At each index of the VCorpus object, there is a PlainTextDocument object, which is essentially a list that contains the actual text data (content), as well as some corresponding metadata (meta).

For example, to examine the contents of the second tweet in coffee_corpus, you'd subset twice. Once to specify the second PlainTextDocument corresponding to the second tweet and again to extract the first (or content) element of that PlainTextDocument:

```{r,warning=FALSE,message=FALSE}
# Make a volatile corpus: coffee_corpus
coffee_corpus <- VCorpus(coffee_source)

# Print out coffee_corpus
coffee_corpus

# Print data on the 15th tweet in coffee_corpus
coffee_corpus[[15]]

# Print the content of the 15th tweet in coffee_corpus
coffee_corpus[[15]][1]

```


#### 2. Dataframe

For Dataframe  there is a Source function called DataframeSource(). The DataframeSource() function treats the entire row as a complete document, so be careful you don't pick up non-text data like customer IDs when sourcing a document this way.

```{r,warning=FALSE,message=FALSE}
# Example_text df

example_text <- data.frame(num = c(1,2,3), 
                           Author1 = c("Text mining is a great time.",
                                       "Text analysis provides insights",
                                       "qdap and tm are used in text mining"),
                           Author2 = c("R is a great language",
                                       "R has many uses",
                                       "DataCamp is cool!"))

example_text[,2:3]
# Create a DataframeSource on columns 2 and 3: df_source
df_source <- DataframeSource(example_text[,2:3])

# Convert df_source to a corpus: df_corpus
df_corpus <- VCorpus(df_source)

# Examine df_corpus
df_corpus

```

### Cleaning and Pre-processing text

#### Common cleaning functions from tm

In bag of words text mining, cleaning helps aggregate terms. For example, it may make sense that the words "miner", "mining" and "mine" should be considered one term. 

Common preprocessing functions include:

1. tolower(): Make all characters lowercase
2. removePunctuation(): Remove all punctuation marks
3. removeNumbers(): Remove numbers
4. stripWhitespace(): Remove excess whitespace

```{r,warning=FALSE,message=FALSE}
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."

# All lowercase
tolower(text)

# Remove punctuation
removePunctuation(text)

# Remove numbers
removeNumbers(text)

# Remove whitespace
stripWhitespace(text)

```

#### Cleaning with qdap

The qdap package offers other text cleaning functions. Each is useful in its own way and is particularly powerful when combined with the others.

1. bracketX(): Remove all text within brackets (e.g. "It's (so) cool" becomes "It's cool")
2. replace_number(): Replace numbers with their word equivalents (e.g. "2" becomes "two")
3. replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. "Sr" becomes "Senior")
4. replace_contraction(): Convert contractions back to their base words (e.g. "shouldn't" becomes "should not")
5. replace_symbol() Replace common symbols with their word equivalents (e.g. "$" becomes "dollar")


```{r,warning=FALSE,message=FALSE}
# Remove text within brackets
bracketX(text)

# Replace numbers with words
replace_number(text)

# Replace abbreviations
replace_abbreviation(text)

# Replace contractions
replace_contraction(text)

# Replace symbols with words
replace_symbol(text)

```

#### All about stop words

Often there are words that are frequent but provide little information. So you may want to remove these so-called stop words. Some common English stop words include "I", "she'll", "the", etc. In the tm package, there are 174 stop words on this common list.

Using the c() function allows you to add new words (separated by commas) to the stop words list.

For eg: all_stops <- c("word1", "word2", stopwords("en"))

Once you have a list of stop words that makes sense, you will use the removeWords() function on your text. removeWords() takes two arguments: the text object to which it's being applied and the list of words to remove.

```{r,warning=FALSE,message=FALSE}
# List standard English stop words
stopwords("en")

# Print text without standard stop words
removeWords(text, stopwords("en"))

# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee","bean",stopwords("en"))

# Remove stop words from text
removeWords(text,new_stops)

```

#### Intro to word stemming and stem completion

The tm package provides the stemDocument() function to get to a word's root. This function either takes in a character vector and returns a character vector, or takes in a PlainTextDocument and returns a PlainTextDocument.

For example,

stemDocument(c("computational", "computers", "computation"))
returns "comput" "comput" "comput"

But because "comput" isn't a real word, we want to re-complete the words so that "computational", "computers", and "computation" all refer to the same word, say "computer", in our ongoing analysis.

We can easily do this with the stemCompletion() function, which takes in a character vector and an argument for the completion dictionary. The completion dictionary can be a character vector or a Corpus object

```{r,warning=FALSE,message=FALSE}
require(tm)
# Create complicate
complicate <- c("complicated","complication","complicatedly")

# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)

# Create the completion dictionary: comp_dict
comp_dict <- "complicate"

# Perform stem completion: complete_text 
complete_text <- stemCompletion(stem_doc,comp_dict)

# Print complete_text
complete_text

```

#### Word stemming and stem completion on a sentence

Let's consider the following sentence as our document for this exercise:

"In a complicated haste, Tom rushed to fix a new complication, too complicatedly."

This sentence contains the same three forms of the word "complicate" that we saw in the previous exercise. The difference here is that even if you called stemDocument() on this sentence, it would return the sentence without stemming any words.

This happens because stemDocument() treats the whole sentence as one word. In other words, our document is a character vector of length 1, instead of length n, where n is the number of words in the document. 

To solve this problem, 
1. we first remove the punctation marks with the removePunctuation()
2. We then strsplit() this character vector of length 1 to length n
3. unlist(), then proceed to stem and re-complete.

```{r,warning=FALSE,message=FALSE}
## text_data

text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."

# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)

# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))

# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)

# Print stem_doc
stem_doc

# Create the completion dictionary: comp_dict
comp_dict <- "complicate"

# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc,comp_dict)

# Print complete_doc
complete_doc

```

#### Apply preprocessing steps to a corpus

The tm package provides a special function tm_map() to apply cleaning functions to a corpus. Mapping these functions to an entire corpus makes scaling the cleaning steps very easy.

To save time (and lines of code) it's a good idea to use a custom function like the one displayed in the editor, since you may be applying the same functions over multiple corpora.

clean_corpus <- function(corpus){  
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))  
  corpus <- tm_map(corpus, removePunctuation)  
  corpus <- tm_map(corpus, removeNumbers)  
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee"))  
  corpus <- tm_map(corpus, content_transformer(tolower))  
  return(corpus)  
} 

tm package functions do not need content_transformer(), but base R and qdap functions do.

Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(coffee_corpus)

### TDM and DTM

Term Document Matrix: Word as rows and documents as columns
Document Term Matrix: Document is a row and word is column (Transform)

The document-term matrix is used when you want to have each document represented as a row. This can be useful if you are comparing authors within rows, or the data is arranged chronologically and you want to preserve the time series.

#### Make a Term Document Matrix

```{r,warning=FALSE,message=FALSE}
coffee_tweets_1 <- iconv(enc2utf8(coffee_tweets),sub="byte")
coffee_source_1 <- VectorSource(coffee_tweets_1)
coffee_corpus_1 <- VCorpus(coffee_source_1)


clean_corpus <- function(corpus){  
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))  
  corpus <- tm_map(corpus, removePunctuation)  
  corpus <- tm_map(corpus, removeNumbers)  
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee"))  
  corpus <- tm_map(corpus, content_transformer(tolower))  
  return(corpus)  
}

clean_corp <- clean_corpus(coffee_corpus_1)

# Create the dtm from the corpus: coffee_dtm
coffee_dtm <- DocumentTermMatrix(clean_corp)

# Print out coffee_dtm data
coffee_dtm

# Convert coffee_dtm to a matrix: coffee_m
coffee_m <- as.matrix(coffee_dtm)

# Print the dimensions of coffee_m
dim(coffee_m)

# Review a portion of the matrix
coffee_m[148:150,2587:2590]

```

#### Make a Document Term Matrix

```{r,warning=FALSE,message=FALSE}
# Create a TDM from clean_corp: coffee_tdm
coffee_tdm <- TermDocumentMatrix(clean_corp)

# Print coffee_tdm data
coffee_tdm

# Convert coffee_tdm to a matrix: coffee_m
coffee_m2 <- as.matrix(coffee_tdm)

# Print the dimensions of the matrix
dim(coffee_m2)

# Review a portion of the matrix
coffee_m2[2587:2590, 148:150]

```

### Visualizations

#### Frequent Terms with tm

In order to analyze TDM or DTM we need to change it to a simple matrix using as.matrix.

1. Calling rowSums() on your newly made matrix aggregates all the terms used in a passage. 
2. Once you have the rowSums(), you can sort() them with decreasing = TRUE, so you can focus on the most common terms.
3. Lastly, you can make a barplot() of the top 5 terms of term_frequency with the following code.
    + barplot(term_frequency[1:5], col = "#C0DE25")

```{r,warning=FALSE,message=FALSE}
## coffee_tdm and coffee_m2 are loaded from previous exercise

# Calculate the rowSums: term_frequency
term_frequency <- rowSums(coffee_m2)

# Sort term_frequency in descending order
term_frequency <- sort(term_frequency,decreasing = TRUE)

# View the top 10 most common words

term_frequency[1:10]
# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las = 2)

```

#### Frequent terms with qdap

```{r,warning=FALSE,message=FALSE}
# Create frequency
frequency <- freq_terms(tweets$text, top = 10, at.least = 3, stopwords = "Top200Words")

# Make a frequency barchart
plot(frequency)

# Create frequency2
frequency2 <- freq_terms(tweets$text, top = 10, at.least = 3, tm::stopwords("english"))

# Make a frequency2 barchart
plot(frequency2)

```

### A simple word cloud

A word cloud is a visualization of terms. In a word cloud, size is often scaled to frequency and in some cases the colors may indicate another measurement. For now, we're keeping it simple: size is related to individual word frequency and we are just selecting a single color.

```{r,warning=FALSE,message=FALSE}
## term_frequency is loaded into your workspace

# Load wordcloud package
require(wordcloud)

# Print the first 10 entries in term_frequency
term_frequency[1:10]

# Create word_freqs
word_freqs <- data.frame(term = names(term_frequency),
                         num = term_frequency)

# Create a wordcloud for the values in word_freqs
wordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = "red")

# Print the 2:10 entries in term_frequency
term_frequency[2:10]

# Create word_freqs
word_freqs_1 <- data.frame(term = names(term_frequency[-1]),
                         num = term_frequency[-1])

# Create a wordcloud for the values in word_freqs
wordcloud(word_freqs_1$term, word_freqs_1$num, max.words = 100, colors = "red")

```

#### Additional Coloring pattern

```{r,warning=FALSE,message=FALSE}

# Create a wordcloud for the values in word_freqs
wordcloud(word_freqs_1$term, word_freqs_1$num, max.words = 100, colors = c("grey80","darkgoldenrod1","tomato"))

# List the available colors
display.brewer.all()

# Create purple_orange
purple_orange <- brewer.pal(10,"PuOr")

# Drop 2 faintest colors
purple_orange <- purple_orange[-(1:2)]

# Create a wordcloud with purple_orange palette
wordcloud(word_freqs_1$term, word_freqs_1$num, max.words = 100, colors = purple_orange)

```


### Find Common Words, Difference in words, Polarized tag cloud

* commonality.cloud() is used to visualize common words across multiple documents.
* Say you want to visualize the words not in common. To do this, you can also use comparison.cloud()
* A commonality.cloud() may be misleading since words could be represented disproportionately in one corpus or the other, even if they are shared. In the commonality cloud, they would show up without telling you which one of the corpora has more term occurrences. To solve this problem, we can create a pyramid.plot() from the plotrix package.

Steps:
1.  To treat the coffee tweets as a single document and likewise for chardonnay, you paste() together all the tweets in each corpus along with the parameter collapse = " ".
2.  This collapses all tweets (separated by a space) into a single vector. Then you can create a vector containing the two collapsed documents.

* Create all_coffee
    + all_coffee <- paste(coffee_tweets$text, collapse = " ")

* Create all_chardonnay
    + all_chardonnay <- paste(chardonnay_tweets$text, collapse = " ")

* Create all_tweets
    + all_tweets <- c(all_coffee,all_chardonnay)

* Convert to a vector source
    + all_tweets <- VectorSource(all_tweets)

* Create all_corpus
    + all_corpus <- VCorpus(all_tweets)
    
* Clean the corpus
    + all_clean <- clean_corpus(all_corpus)

* Create all_tdm
    + all_tdm <- TermDocumentMatrix(all_clean)

#### For Visualizing common words

* Create all_m
    + all_m <- as.matrix(all_tdm)

* Print a commonality cloud
    + commonality.cloud(all_m,max.words=100,colors="steelblue1")

#### For Visualizing difference in words

* Give the columns distinct names
    + colnames(all_tdm) <- c("coffee","chardonnay")

* Create all_m
    + all_m <- as.matrix(all_tdm)

* Create comparison cloud
    + comparison.cloud(all_m,max.words=50,colors=c("orange","blue"))

#### For Visualizing pyramid
* Create common_words
    + common_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)

* Create difference
    + difference <- abs(common_words[, 1] - common_words[, 2])

* Combine common_words and difference
    + common_words <- cbind(common_words, difference)

* Order the data frame from most differences to least
    + common_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]

* Create top25_df
    + top25_df <- data.frame(x = common_words[1:25, 1], 
    +                        y = common_words[1:25, 2], 
    +                        labels = rownames(common_words[1:25, ]))

* Create the pyramid plot
    + pyramid.plot(top25_df$x, top25_df$y,
    +              labels = top25_df$labels, gap = 8,
    +              top.labels = c("Chardonnay", "Words", "Coffee"),
    +              main = "Words in Common", laxlab = NULL, 
    +              raxlab = NULL, unit = NULL)


### Visualize word networks

```{r,warning=FALSE,message=FALSE}

# Word association
word_associate(coffee_tweets_1, match.string = c("barista"), 
               stopwords = c(Top200Words, "coffee", "amp"), 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))
# Add title
title(main = "Barista Coffee Tweet Associations")

```

### Dendograms

#### Distance matrix and dendrogram

A simple way to do word cluster analysis is with a dendrogram on your term-document matrix. 

Steps to execute:

1. Once you have a TDM, you can call dist() to compute the differences between each row of the matrix.
2. Next, you call hclust() to perform cluster analysis on the dissimilarities of the distance matrix.
3.  Lastly, you can visualize the word frequency distances using a dendrogram and plot()

```{r,warning=FALSE,message=FALSE}

## Creating rain dataframe

rain <- data.frame(city = c("Cleveland","Portland","Boston","New Orleans"),
                   rainfall = c(39.14,39.14,43.77,62.45))

# Create dist_rain
dist_rain <- dist(rain[,2])

# View the distance matrix
dist_rain

# Create hc
hc <- hclust(dist_rain)

# Plot hc
plot(hc,labels = rain$city)

```

#### Make a distance matrix and dendrogram from a TDM

Now that you understand the steps in making a dendrogram, you can apply them to text. But first, you have to limit the number of words in your TDM using removeSparseTerms() from tm. Why would you want to adjust the sparsity of the TDM/DTM?

TDMs and DTMs are sparse, meaning they contain mostly zeros. Remember that 1000 tweets can become a TDM with over 3000 terms! You won't be able to easily interpret a dendrogram that is so cluttered, especially if you are working on more text.

A good TDM has between 25 and 70 terms. The lower the sparse value, the more terms are kept. The closer it is to 1, the fewer are kept. This value is a percentage cutoff of zeros for each term in the TDM.

```{r,warning=FALSE,message=FALSE}
# Print the dimensions of coffee_tdm
dim(coffee_tdm)

# Create tdm1
tdm1 <- removeSparseTerms(coffee_tdm,sparse = 0.95)

# Create tdm2
tdm2 <- removeSparseTerms(coffee_tdm,sparse = 0.975)

# Print tdm1
tdm1

# Print tdm2
tdm2

# Create tdm_m
tdm_m <- as.matrix(tdm2)

# Create tdm_df
tdm_df <- as.data.frame(tdm_m)

# Create tweets_dist
tweets_dist <- dist(tdm_df)

# Create hc
hc <- hclust(tweets_dist)

# Plot the dendrogram
plot(hc)

## To make plot more smooth we may need dendextend package
require(dendextend)

# Create hcd
hcd <- as.dendrogram(hc)

# Print the labels in hcd
labels(hcd)

# Change the branch color to red for "marvin" and "gaye"
hcd <- branches_attr_by_labels(hcd,c("starbucks", "think"),color="red")

# Plot hcd
plot(hcd, main = "Better Dendogram")

# Add cluster rectangles 
rect.dendrogram(hcd,k=2,border="grey50")

```
